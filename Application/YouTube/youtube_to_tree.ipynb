{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From youtube to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP words source:\n",
    "# title, tags(\"taga\"|\"tagb\"), description\n",
    "\n",
    "# herarcial sources:\n",
    "# channel_title, category, tags\n",
    "\n",
    "# boolean statistical columns\n",
    "# existance tags\\words, comments_disabled, ratings_disabled, video_error_or_removed\n",
    "\n",
    "# also possible - ancestral reconstrion for continues veriables. these bring into play:\n",
    "# views, likess, dislikes, comment_count\n",
    "\n",
    "# implemented in R here: http://www.phytools.org/eqg/Exercise_5.2/\n",
    "# BayesTraits has BayesContinuous also\n",
    "# This looks very promissing:\n",
    "# https://github.com/michaelgruenstaeudl/WARACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible questions:\n",
    "\n",
    "# which tags are in the same semantic felid?\n",
    "# what tag is most correlated with view/comment/likes/(like/dislike ratio) etc.\n",
    "# what tag's are assosiated with like/dislike ratio (or binirize the ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workplan\n",
    "* clean tag and description text, and use tokenize pipe\n",
    "* create sparse \"token over vid\" matrix - (include - categories columns !!!)\n",
    "* herarchial clustering based on sparse onehot matrix (cat, tag, channel)\n",
    "* ancestral reconstruction of traits: useing MaxParsimony for sparse tokens\n",
    "* Bonus: ancestral reconstruction of cont traits - using BayesContinuous of the 5 continues columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Reconstrtuction libraries\n",
    "import scipy.sparse as sp\n",
    "import ete3\n",
    "from Analytical_solution_test_parallel import SetHomoplasy\n",
    "\n",
    "# nlp libraries\n",
    "from process_wikipedia import remove_special_chars, remove_html_tags, clean_string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vids without likes or dislikes are wierd \n",
    "us_videos_first = pd.read_parquet('youtube_dataset/USfirst.parq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vids without likes or dislikes are wierd \n",
    "us_videos_first = us_videos_first[(us_videos_first[\"likes\"] > 0) & (us_videos_first[\"dislikes\"] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_videos_first.index.name = 'original_index'\n",
    "us_videos_first = us_videos_first.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleanup(text):\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_chars(text, ['\\n', '–']+list(punctuation))\n",
    "    text = clean_string(text, set(stopwords.words('english')))\n",
    "    return text\n",
    "\n",
    "stemm = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "def tokanizer(text, ret_trans=True, unique=False):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words, inv_words = np.unique(words, return_inverse=True)\n",
    "    word_trans_ = dict.fromkeys(words)\n",
    "    for i in range(len(words)): \n",
    "        word_trans_[words[i]] = stemm.stem(lemma.lemmatize(words[i]))\n",
    "        words[i] = word_trans_[words[i]]\n",
    "    \n",
    "    tokens, inv_tokens = np.unique(words, return_inverse=True)\n",
    "    if not unique: tokens = (tokens[inv_tokens])[inv_words]\n",
    "    tokens = tokens.tolist()\n",
    "    if ret_trans:\n",
    "        return (tokens, word_trans_)\n",
    "    else:\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim import matutils\n",
    "def SetWordDataDictTable(table, no_below=3, full_rec=False):\n",
    "    # collect tokens and translations\n",
    "    table['tokens'] = None\n",
    "    token_corp = Dictionary()\n",
    "    word_to_token = dict()\n",
    "    diverse_dict = dict()\n",
    "    for i,row in tqdm.tqdm_notebook(table.iterrows(), total=len(table), desc='Collecting vid Tokens'):\n",
    "        # cleanup\n",
    "        text = text_cleanup(row['tags'].replace('|',' '))\n",
    "        tokens, trans = tokanizer(text, unique=True, ret_trans=True)\n",
    "        # diverse track\n",
    "        for token in tokens:\n",
    "            if token == '': continue\n",
    "            diverse_dict[token] = diverse_dict.get(token, [])+[row['channel_title']]\n",
    "        # placement\n",
    "        table.loc[i, 'tokens'] = ' '.join(tokens)\n",
    "        word_to_token.update(trans)\n",
    "        token_corp.add_documents([table.loc[i, 'tokens'].split(' ')])\n",
    "\n",
    "    # filter token space to relevant tokens\n",
    "    token_corp.filter_extremes(no_below=no_below)\n",
    "    \n",
    "    # filter token space to diverse tokens only\n",
    "    for k in diverse_dict:\n",
    "        diverse_dict[k] = len(set(diverse_dict[k]))\n",
    "    \n",
    "    # at least 2 channels using a token... - we have \"channel title\" for the rest.\n",
    "    tag_diverse_series = pd.Series(diverse_dict)\n",
    "    un_usable_tags = tag_diverse_series[tag_diverse_series<2].index.tolist()\n",
    "    token_corp.filter_tokens(token_corp.doc2idx(un_usable_tags))\n",
    "    \n",
    "    # token_to_word inv map\n",
    "    dat_size = len(token_corp)\n",
    "    table.token_corp = token_corp\n",
    "    table.word_to_token = word_to_token\n",
    "    table.token_to_word = dict()\n",
    "    for k, v in word_to_token.items():\n",
    "        table.token_to_word[v] = table.token_to_word.get(v, [])\n",
    "        table.token_to_word[v].append(k)\n",
    "    \n",
    "    # word2vec mapping\n",
    "    sparse_tokens = []\n",
    "    for i, row in tqdm.tqdm_notebook(table.iterrows(), total=len(table), desc='Token2Vec'):\n",
    "        cols = np.array(token_corp.doc2idx(row['tokens'].split(' ')))\n",
    "        cols = cols[cols>-1]\n",
    "        rows = [0]*len(cols)\n",
    "        dat = [1]*len(cols)\n",
    "        sparse_tokens.append(sp.csr_matrix((dat, (rows, cols)), shape=(1, dat_size)))\n",
    "    sparse_tokens = sp.vstack(sparse_tokens)\n",
    "    return table, sparse_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparse rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e113f05e3b422db337e1d516a3a904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Collecting vid Tokens', max=6244, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/kishonylab/KishonyStorage/galn/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning:\n",
      "\n",
      "Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "\n",
      "/media/kishonylab/KishonyStorage/galn/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning:\n",
      "\n",
      "Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "\n",
      "/media/kishonylab/KishonyStorage/galn/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning:\n",
      "\n",
      "Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04945d5ef1e4389bcdae67a19190f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Token2Vec', max=6244, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "us_videos_first, sparse_tokens = SetWordDataDictTable(us_videos_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_channels = sp.csr_matrix(pd.get_dummies(us_videos_first['channel_title'], sparse=True).values)\n",
    "sparse_channels_rep = sparse_channels[:, np.argwhere(sparse_channels.sum(axis=0) > 1)[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cat = sp.csr_matrix(pd.get_dummies(us_videos_first['category_id']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_map = sp.hstack([sparse_cat, sparse_channels_rep, sparse_tokens]) # todo add tokens...\n",
    "quality_map_no_tag = sp.hstack([sparse_cat, sparse_channels_rep]) # todo add tokens..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree reconstrtuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "dist_mat = pairwise.cosine_distances(quality_map)\n",
    "dist_mat_no_tags = pairwise.cosine_distances(quality_map_no_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TreeOperations import TreeFromDist\n",
    "recon_tree = TreeFromDist(dist_mat, colnames=us_videos_first.index.tolist())\n",
    "recon_tree_no_tags = TreeFromDist(dist_mat_no_tags, colnames=us_videos_first.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state reconstrtuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxParsimony(X, Tree, tip_to_row):\n",
    "    # 2 represents {0,1} set\n",
    "    sp_to_arr = lambda sp_arr: np.array(sp_arr.todense().astype(np.int8))[0]\n",
    "    wrap = lambda x: sp_to_arr(X[tip_to_row(x.name)]) if x.is_leaf() else sp_to_arr(x.dat)\n",
    "    Tree.size = 0\n",
    "    for _ in Tree.traverse(): Tree.size += 1\n",
    "    for i, node in tqdm.tqdm(enumerate(Tree.traverse('postorder')), total=Tree.size, desc='Ancestral Reconstruction: 1st pass'):\n",
    "        if node.is_leaf():\n",
    "            node.dat = X[tip_to_row(node.name)]\n",
    "            continue\n",
    "        # node.name = i\n",
    "        children = [wrap(c) for c in node.children]\n",
    "        res = children[0].copy()\n",
    "        eq = np.equal(*children)\n",
    "        res[children[0] == 2] = children[1][children[0] == 2]  # 2 is the union {0,1}\n",
    "        res[children[1] == 2] = children[0][children[1] == 2]\n",
    "        res[(children[0] != 2) & (children[1] != 2) & ~eq] = 2\n",
    "        node.dat = sp.csr_matrix(res)\n",
    "\n",
    "    post = Tree.traverse('preorder')\n",
    "    root = next(post)\n",
    "    root.random = sp.csr_matrix((wrap(root) == 2))\n",
    "    root.dat[root.dat == 2] = np.random.choice([1, 0], size=(root.dat == 2).sum())\n",
    "    for node in tqdm.tqdm(post, total=Tree.size - 1, desc='Ancestral Reconstruction: 2nd pass'):\n",
    "        if node.is_leaf(): \n",
    "            node.random = sp.csr_matrix((node.up.random) * 0)\n",
    "            continue\n",
    "        parent_ = wrap(node.up)\n",
    "        node_ = wrap(node)\n",
    "        res = node_.copy()\n",
    "        res[node_ == 2] = parent_[node_ == 2]\n",
    "        node.random = sp.csr_matrix((node.up.random).multiply(sp.csr_matrix(node_) == 2))  # these are unstable positions - will not be counted\n",
    "        node.dat = sp.csr_matrix(res)\n",
    "\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TreeOperations import *\n",
    "pickler = Tree_pickler()\n",
    "saved_attr = ['token_corp', 'word_to_token', 'token_to_word', 'homoplasy', 'homoplasy_hist', 'size']\n",
    "sp_list = ['dat', 'random']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ancestral Reconstruction: 1st pass: 100%|██████████| 12487/12487 [00:03<00:00, 3428.76it/s]\n",
      "Ancestral Reconstruction: 2nd pass: 100%|██████████| 12486/12486 [00:05<00:00, 2204.67it/s]\n",
      "100%|█████████▉| 12486/12488 [00:04<00:00, 2623.87it/s]\n"
     ]
    }
   ],
   "source": [
    "recon_tree = MaxParsimony(sparse_tokens, recon_tree, int)\n",
    "recon_tree = SetHomoplasy(recon_tree)\n",
    "# inherit\n",
    "recon_tree.token_corp = us_videos_first.token_corp\n",
    "recon_tree.word_to_token = us_videos_first.word_to_token\n",
    "recon_tree.token_to_word = us_videos_first.token_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler.saveTree(recon_tree, 'youtube_dataset/Saved_tree', saved_attr, sp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/o tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ancestral Reconstruction: 1st pass: 100%|██████████| 12487/12487 [00:03<00:00, 3393.93it/s]\n",
      "Ancestral Reconstruction: 2nd pass: 100%|██████████| 12486/12486 [00:05<00:00, 2172.89it/s]\n",
      "100%|█████████▉| 12486/12488 [00:04<00:00, 2668.32it/s]\n"
     ]
    }
   ],
   "source": [
    "recon_tree_no_tags = MaxParsimony(sparse_tokens, recon_tree_no_tags, int)\n",
    "recon_tree_no_tags = SetHomoplasy(recon_tree_no_tags)\n",
    "# inherit\n",
    "recon_tree_no_tags.token_corp = us_videos_first.token_corp\n",
    "recon_tree_no_tags.word_to_token = us_videos_first.word_to_token\n",
    "recon_tree_no_tags.token_to_word = us_videos_first.token_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler.saveTree(recon_tree_no_tags, 'youtube_dataset/Saved_tree_no_tags', saved_attr, sp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
